{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFVxWZGJxprU"
   },
   "source": [
    "# CS4001/4042 Assignment 1, Part B, Q2\n",
    "In Question B1, we used the Category Embedding model. This creates a feedforward neural network in which the categorical features get learnable embeddings. In this question, we will make use of a library called Pytorch-WideDeep. This library makes it easy to work with multimodal deep-learning problems combining images, text, and tables. We will just be utilizing the deeptabular component of this library through the TabMlp network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EycCozG06Duu"
   },
   "outputs": [],
   "source": [
    "# !pip install pytorch-widedeep\n",
    "# !pip install --upgrade torch --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lq0elU0J53Yo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\XIANG\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: [WinError 127] The specified procedure could not be found\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "<frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "\n",
    "import os\n",
    "\n",
    "import random\n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pytorch_widedeep.preprocessing import TabPreprocessor\n",
    "from pytorch_widedeep.models import TabMlp, WideDeep\n",
    "from pytorch_widedeep import Trainer\n",
    "from pytorch_widedeep.metrics import R2Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aU3xdVpwzuLx"
   },
   "source": [
    ">Divide the dataset (‘hdb_price_prediction.csv’) into train and test sets by using entries from the year 2020 and before as training data, and entries from 2021 and after as the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_oYG6lNIh7Mp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('hdb_price_prediction.csv')\n",
    "\n",
    "# TODO: Enter your code here\n",
    "train_data = df[df['year'] <= 2020]\n",
    "test_data = df[df['year'] >= 2021]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_q9PoR50JAA"
   },
   "source": [
    ">Refer to the documentation of Pytorch-WideDeep and perform the following tasks:\n",
    "https://pytorch-widedeep.readthedocs.io/en/latest/index.html\n",
    "* Use [**TabPreprocessor**](https://pytorch-widedeep.readthedocs.io/en/latest/examples/01_preprocessors_and_utils.html#2-tabpreprocessor) to create the deeptabular component using the continuous\n",
    "features and the categorical features. Use this component to transform the training dataset.\n",
    "* Create the [**TabMlp**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/model_components.html#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp) model with 2 linear layers in the MLP, with 200 and 100 neurons respectively.\n",
    "* Create a [**Trainer**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/trainer.html#pytorch_widedeep.training.Trainer) for the training of the created TabMlp model with the root mean squared error (RMSE) cost function. Train the model for 100 epochs using this trainer, keeping a batch size of 64. (Note: set the *num_workers* parameter to 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ZBY1iqUXtYWn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\XIANG\\AppData\\Roaming\\Python\\Python310\\site-packages\\pytorch_widedeep\\preprocessing\\tab_preprocessor.py:334: UserWarning: Continuous columns will not be normalised\n",
      "  warnings.warn(\"Continuous columns will not be normalised\")\n",
      "epoch 1: 100%|██████████| 1366/1366 [00:19<00:00, 69.96it/s, loss=2.03e+5, metrics={'r2': -1.6901}]\n",
      "epoch 2: 100%|██████████| 1366/1366 [00:21<00:00, 62.35it/s, loss=8.14e+4, metrics={'r2': 0.6858}]\n",
      "epoch 3: 100%|██████████| 1366/1366 [00:23<00:00, 59.13it/s, loss=7.21e+4, metrics={'r2': 0.7667}]\n",
      "epoch 4: 100%|██████████| 1366/1366 [00:28<00:00, 48.33it/s, loss=6.88e+4, metrics={'r2': 0.7893}]\n",
      "epoch 5: 100%|██████████| 1366/1366 [00:22<00:00, 61.10it/s, loss=6.69e+4, metrics={'r2': 0.8012}]\n",
      "epoch 6: 100%|██████████| 1366/1366 [00:21<00:00, 63.71it/s, loss=6.56e+4, metrics={'r2': 0.8091}]\n",
      "epoch 7: 100%|██████████| 1366/1366 [00:26<00:00, 52.09it/s, loss=6.47e+4, metrics={'r2': 0.814}] \n",
      "epoch 8: 100%|██████████| 1366/1366 [00:24<00:00, 56.71it/s, loss=6.39e+4, metrics={'r2': 0.8184}]\n",
      "epoch 9: 100%|██████████| 1366/1366 [00:29<00:00, 46.05it/s, loss=6.34e+4, metrics={'r2': 0.8209}]\n",
      "epoch 10: 100%|██████████| 1366/1366 [00:28<00:00, 48.16it/s, loss=6.27e+4, metrics={'r2': 0.8247}]\n",
      "epoch 11: 100%|██████████| 1366/1366 [00:23<00:00, 57.31it/s, loss=6.24e+4, metrics={'r2': 0.8263}]\n",
      "epoch 12: 100%|██████████| 1366/1366 [00:22<00:00, 59.72it/s, loss=6.22e+4, metrics={'r2': 0.8271}]\n",
      "epoch 13: 100%|██████████| 1366/1366 [00:25<00:00, 53.49it/s, loss=6.2e+4, metrics={'r2': 0.8283}] \n",
      "epoch 14: 100%|██████████| 1366/1366 [00:25<00:00, 53.70it/s, loss=6.16e+4, metrics={'r2': 0.8304}]\n",
      "epoch 15: 100%|██████████| 1366/1366 [00:28<00:00, 47.53it/s, loss=6.15e+4, metrics={'r2': 0.8306}]\n",
      "epoch 16: 100%|██████████| 1366/1366 [00:25<00:00, 54.38it/s, loss=6.12e+4, metrics={'r2': 0.8323}]\n",
      "epoch 17: 100%|██████████| 1366/1366 [00:22<00:00, 60.20it/s, loss=6.11e+4, metrics={'r2': 0.8326}]\n",
      "epoch 18: 100%|██████████| 1366/1366 [00:22<00:00, 60.60it/s, loss=6.09e+4, metrics={'r2': 0.834}] \n",
      "epoch 19: 100%|██████████| 1366/1366 [00:23<00:00, 58.06it/s, loss=6.07e+4, metrics={'r2': 0.8348}]\n",
      "epoch 20: 100%|██████████| 1366/1366 [00:24<00:00, 55.56it/s, loss=6.07e+4, metrics={'r2': 0.8349}]\n",
      "epoch 21: 100%|██████████| 1366/1366 [00:22<00:00, 60.76it/s, loss=6.04e+4, metrics={'r2': 0.8363}]\n",
      "epoch 22: 100%|██████████| 1366/1366 [00:27<00:00, 49.79it/s, loss=6.03e+4, metrics={'r2': 0.8366}]\n",
      "epoch 23: 100%|██████████| 1366/1366 [00:20<00:00, 65.67it/s, loss=6.03e+4, metrics={'r2': 0.8365}]\n",
      "epoch 24: 100%|██████████| 1366/1366 [00:21<00:00, 64.30it/s, loss=6e+4, metrics={'r2': 0.8384}]   \n",
      "epoch 25: 100%|██████████| 1366/1366 [00:21<00:00, 64.97it/s, loss=5.99e+4, metrics={'r2': 0.8387}]\n",
      "epoch 26: 100%|██████████| 1366/1366 [00:20<00:00, 66.68it/s, loss=6e+4, metrics={'r2': 0.8382}]   \n",
      "epoch 27: 100%|██████████| 1366/1366 [00:20<00:00, 66.78it/s, loss=6e+4, metrics={'r2': 0.8385}]   \n",
      "epoch 28: 100%|██████████| 1366/1366 [00:22<00:00, 59.69it/s, loss=5.97e+4, metrics={'r2': 0.8398}]\n",
      "epoch 29: 100%|██████████| 1366/1366 [00:23<00:00, 58.16it/s, loss=5.99e+4, metrics={'r2': 0.839}] \n",
      "epoch 30: 100%|██████████| 1366/1366 [00:24<00:00, 56.87it/s, loss=5.96e+4, metrics={'r2': 0.8404}]\n",
      "epoch 31: 100%|██████████| 1366/1366 [00:21<00:00, 62.48it/s, loss=5.96e+4, metrics={'r2': 0.8402}]\n",
      "epoch 32: 100%|██████████| 1366/1366 [00:21<00:00, 62.56it/s, loss=5.95e+4, metrics={'r2': 0.8405}]\n",
      "epoch 33: 100%|██████████| 1366/1366 [00:21<00:00, 64.60it/s, loss=5.95e+4, metrics={'r2': 0.8409}]\n",
      "epoch 34: 100%|██████████| 1366/1366 [00:21<00:00, 64.57it/s, loss=5.95e+4, metrics={'r2': 0.8408}]\n",
      "epoch 35: 100%|██████████| 1366/1366 [00:21<00:00, 64.70it/s, loss=5.94e+4, metrics={'r2': 0.8409}]\n",
      "epoch 36: 100%|██████████| 1366/1366 [00:21<00:00, 64.38it/s, loss=5.94e+4, metrics={'r2': 0.841}] \n",
      "epoch 37: 100%|██████████| 1366/1366 [00:21<00:00, 64.66it/s, loss=5.92e+4, metrics={'r2': 0.8419}]\n",
      "epoch 38: 100%|██████████| 1366/1366 [00:22<00:00, 61.86it/s, loss=5.93e+4, metrics={'r2': 0.8421}]\n",
      "epoch 39: 100%|██████████| 1366/1366 [00:22<00:00, 60.78it/s, loss=5.92e+4, metrics={'r2': 0.8425}]\n",
      "epoch 40: 100%|██████████| 1366/1366 [00:22<00:00, 60.42it/s, loss=5.91e+4, metrics={'r2': 0.8424}]\n",
      "epoch 41: 100%|██████████| 1366/1366 [00:25<00:00, 54.46it/s, loss=5.92e+4, metrics={'r2': 0.8418}]\n",
      "epoch 42: 100%|██████████| 1366/1366 [00:22<00:00, 60.57it/s, loss=5.89e+4, metrics={'r2': 0.8434}]\n",
      "epoch 43: 100%|██████████| 1366/1366 [00:22<00:00, 61.89it/s, loss=5.92e+4, metrics={'r2': 0.842}] \n",
      "epoch 44: 100%|██████████| 1366/1366 [00:29<00:00, 46.86it/s, loss=5.91e+4, metrics={'r2': 0.8426}]\n",
      "epoch 45: 100%|██████████| 1366/1366 [00:30<00:00, 44.11it/s, loss=5.89e+4, metrics={'r2': 0.8438}]\n",
      "epoch 46: 100%|██████████| 1366/1366 [00:22<00:00, 61.04it/s, loss=5.89e+4, metrics={'r2': 0.8434}]\n",
      "epoch 47: 100%|██████████| 1366/1366 [00:21<00:00, 63.52it/s, loss=5.89e+4, metrics={'r2': 0.8438}]\n",
      "epoch 48: 100%|██████████| 1366/1366 [00:22<00:00, 60.69it/s, loss=5.89e+4, metrics={'r2': 0.8433}]\n",
      "epoch 49: 100%|██████████| 1366/1366 [00:23<00:00, 57.89it/s, loss=5.89e+4, metrics={'r2': 0.8436}]\n",
      "epoch 50: 100%|██████████| 1366/1366 [00:25<00:00, 53.73it/s, loss=5.88e+4, metrics={'r2': 0.844}] \n",
      "epoch 51: 100%|██████████| 1366/1366 [00:22<00:00, 60.19it/s, loss=5.88e+4, metrics={'r2': 0.8444}]\n",
      "epoch 52: 100%|██████████| 1366/1366 [00:23<00:00, 58.67it/s, loss=5.87e+4, metrics={'r2': 0.8444}]\n",
      "epoch 53: 100%|██████████| 1366/1366 [00:22<00:00, 60.35it/s, loss=5.86e+4, metrics={'r2': 0.845}] \n",
      "epoch 54: 100%|██████████| 1366/1366 [00:21<00:00, 62.32it/s, loss=5.87e+4, metrics={'r2': 0.8447}]\n",
      "epoch 55: 100%|██████████| 1366/1366 [00:18<00:00, 75.51it/s, loss=5.86e+4, metrics={'r2': 0.8448}]\n",
      "epoch 56: 100%|██████████| 1366/1366 [00:20<00:00, 67.02it/s, loss=5.87e+4, metrics={'r2': 0.8449}]\n",
      "epoch 57: 100%|██████████| 1366/1366 [00:19<00:00, 69.14it/s, loss=5.87e+4, metrics={'r2': 0.8445}]\n",
      "epoch 58: 100%|██████████| 1366/1366 [00:19<00:00, 70.61it/s, loss=5.86e+4, metrics={'r2': 0.8449}]\n",
      "epoch 59: 100%|██████████| 1366/1366 [00:19<00:00, 69.17it/s, loss=5.86e+4, metrics={'r2': 0.8448}]\n",
      "epoch 60: 100%|██████████| 1366/1366 [00:20<00:00, 66.62it/s, loss=5.86e+4, metrics={'r2': 0.8447}]\n",
      "epoch 61: 100%|██████████| 1366/1366 [00:21<00:00, 64.33it/s, loss=5.85e+4, metrics={'r2': 0.8453}]\n",
      "epoch 62: 100%|██████████| 1366/1366 [00:20<00:00, 65.26it/s, loss=5.85e+4, metrics={'r2': 0.8454}]\n",
      "epoch 63: 100%|██████████| 1366/1366 [00:21<00:00, 64.15it/s, loss=5.84e+4, metrics={'r2': 0.8458}]\n",
      "epoch 64: 100%|██████████| 1366/1366 [00:20<00:00, 65.92it/s, loss=5.83e+4, metrics={'r2': 0.8464}]\n",
      "epoch 65: 100%|██████████| 1366/1366 [00:21<00:00, 63.07it/s, loss=5.83e+4, metrics={'r2': 0.8464}]\n",
      "epoch 66: 100%|██████████| 1366/1366 [00:20<00:00, 67.53it/s, loss=5.86e+4, metrics={'r2': 0.8449}]\n",
      "epoch 67: 100%|██████████| 1366/1366 [00:21<00:00, 62.28it/s, loss=5.84e+4, metrics={'r2': 0.8461}]\n",
      "epoch 68: 100%|██████████| 1366/1366 [00:19<00:00, 69.10it/s, loss=5.83e+4, metrics={'r2': 0.8465}]\n",
      "epoch 69: 100%|██████████| 1366/1366 [00:23<00:00, 57.50it/s, loss=5.83e+4, metrics={'r2': 0.8465}]\n",
      "epoch 70: 100%|██████████| 1366/1366 [00:20<00:00, 67.51it/s, loss=5.82e+4, metrics={'r2': 0.8472}]\n",
      "epoch 71: 100%|██████████| 1366/1366 [00:19<00:00, 69.61it/s, loss=5.82e+4, metrics={'r2': 0.8468}]\n",
      "epoch 72: 100%|██████████| 1366/1366 [00:18<00:00, 73.14it/s, loss=5.82e+4, metrics={'r2': 0.8468}]\n",
      "epoch 73: 100%|██████████| 1366/1366 [00:22<00:00, 60.70it/s, loss=5.81e+4, metrics={'r2': 0.8477}]\n",
      "epoch 74: 100%|██████████| 1366/1366 [00:19<00:00, 71.55it/s, loss=5.81e+4, metrics={'r2': 0.8472}]\n",
      "epoch 75: 100%|██████████| 1366/1366 [00:19<00:00, 68.97it/s, loss=5.82e+4, metrics={'r2': 0.8472}]\n",
      "epoch 76: 100%|██████████| 1366/1366 [00:18<00:00, 72.17it/s, loss=5.8e+4, metrics={'r2': 0.8476}] \n",
      "epoch 77: 100%|██████████| 1366/1366 [00:18<00:00, 73.43it/s, loss=5.81e+4, metrics={'r2': 0.8472}]\n",
      "epoch 78: 100%|██████████| 1366/1366 [00:18<00:00, 72.15it/s, loss=5.8e+4, metrics={'r2': 0.8477}] \n",
      "epoch 79: 100%|██████████| 1366/1366 [00:18<00:00, 73.71it/s, loss=5.81e+4, metrics={'r2': 0.8472}]\n",
      "epoch 80: 100%|██████████| 1366/1366 [00:20<00:00, 66.09it/s, loss=5.79e+4, metrics={'r2': 0.848}] \n",
      "epoch 81: 100%|██████████| 1366/1366 [00:19<00:00, 69.80it/s, loss=5.81e+4, metrics={'r2': 0.8474}]\n",
      "epoch 82: 100%|██████████| 1366/1366 [00:19<00:00, 69.32it/s, loss=5.8e+4, metrics={'r2': 0.8479}] \n",
      "epoch 83: 100%|██████████| 1366/1366 [00:19<00:00, 68.35it/s, loss=5.79e+4, metrics={'r2': 0.8486}]\n",
      "epoch 84: 100%|██████████| 1366/1366 [00:20<00:00, 67.58it/s, loss=5.78e+4, metrics={'r2': 0.8485}]\n",
      "epoch 85: 100%|██████████| 1366/1366 [00:20<00:00, 68.18it/s, loss=5.77e+4, metrics={'r2': 0.8491}]\n",
      "epoch 86: 100%|██████████| 1366/1366 [00:21<00:00, 63.44it/s, loss=5.79e+4, metrics={'r2': 0.8483}]\n",
      "epoch 87: 100%|██████████| 1366/1366 [00:19<00:00, 69.36it/s, loss=5.76e+4, metrics={'r2': 0.8497}]\n",
      "epoch 88: 100%|██████████| 1366/1366 [00:20<00:00, 66.32it/s, loss=5.76e+4, metrics={'r2': 0.8497}]\n",
      "epoch 89: 100%|██████████| 1366/1366 [00:20<00:00, 66.40it/s, loss=5.78e+4, metrics={'r2': 0.849}] \n",
      "epoch 90: 100%|██████████| 1366/1366 [00:20<00:00, 66.95it/s, loss=5.77e+4, metrics={'r2': 0.8493}]\n",
      "epoch 91: 100%|██████████| 1366/1366 [00:20<00:00, 66.67it/s, loss=5.76e+4, metrics={'r2': 0.8497}]\n",
      "epoch 92: 100%|██████████| 1366/1366 [00:22<00:00, 61.94it/s, loss=5.78e+4, metrics={'r2': 0.8487}]\n",
      "epoch 93: 100%|██████████| 1366/1366 [00:21<00:00, 64.23it/s, loss=5.75e+4, metrics={'r2': 0.8505}]\n",
      "epoch 94: 100%|██████████| 1366/1366 [00:19<00:00, 69.73it/s, loss=5.75e+4, metrics={'r2': 0.8501}]\n",
      "epoch 95: 100%|██████████| 1366/1366 [00:20<00:00, 66.63it/s, loss=5.75e+4, metrics={'r2': 0.8505}]\n",
      "epoch 96: 100%|██████████| 1366/1366 [00:20<00:00, 67.63it/s, loss=5.75e+4, metrics={'r2': 0.85}]  \n",
      "epoch 97: 100%|██████████| 1366/1366 [00:20<00:00, 67.59it/s, loss=5.73e+4, metrics={'r2': 0.8513}]\n",
      "epoch 98: 100%|██████████| 1366/1366 [00:18<00:00, 75.23it/s, loss=5.76e+4, metrics={'r2': 0.8499}]\n",
      "epoch 99: 100%|██████████| 1366/1366 [00:19<00:00, 71.25it/s, loss=5.74e+4, metrics={'r2': 0.8509}]\n",
      "epoch 100: 100%|██████████| 1366/1366 [00:19<00:00, 71.68it/s, loss=5.74e+4, metrics={'r2': 0.8511}]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Enter your code here\n",
    "categorical_cols = [('month', df['month'].nunique()), ('town', df['town'].nunique()), ('flat_model_type', df['flat_model_type'].nunique()), ('storey_range', df['storey_range'].nunique())]\n",
    "continuous_cols = ['dist_to_nearest_stn', 'dist_to_dhoby', 'degree_centrality', 'eigenvector_centrality', 'remaining_lease_years', 'floor_area_sqm']\n",
    "\n",
    "# cat_embed_cols = [(column_name, embed_dim), ...]\n",
    "tab_preprocessor = TabPreprocessor(cat_embed_cols=categorical_cols, continuous_cols=continuous_cols)\n",
    "\n",
    "X_tab = tab_preprocessor.fit_transform(train_data)\n",
    "\n",
    "#TabMlp(column_idx, cat_embed_input=None, cat_embed_dropout=0.1, use_cat_bias=False, cat_embed_activation=None, continuous_cols=None, cont_norm_layer='batchnorm', embed_continuous=False, cont_embed_dim=32, cont_embed_dropout=0.1, use_cont_bias=True, cont_embed_activation=None, mlp_hidden_dims=[200, 100], mlp_activation='relu', mlp_dropout=0.1, mlp_batchnorm=False, mlp_batchnorm_last=False, mlp_linear_first=False)\n",
    "\n",
    "#https://pytorch-widedeep.readthedocs.io/en/latest/quick_start.html (reference code from here)\n",
    "tab_mlp = TabMlp(\n",
    "    column_idx=tab_preprocessor.column_idx, \n",
    "    cat_embed_input=tab_preprocessor.cat_embed_input,\n",
    "    continuous_cols=continuous_cols,\n",
    "    mlp_hidden_dims=[200, 100]\n",
    ")\n",
    "\n",
    "model = WideDeep(deeptabular=tab_mlp)\n",
    "\n",
    "#train and validate\n",
    "trainer = Trainer(model, cost_fn=\"rmse\",num_workers=0, metrics=[R2Score])\n",
    "trainer.fit(\n",
    "    X_tab=X_tab,\n",
    "    target=train_data['resale_price'].values,\n",
    "    n_epochs=100,\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V46s-MdM0y5c"
   },
   "source": [
    ">Report the test RMSE and the test R2 value that you obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "KAhAgvMC07g6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 100%|██████████| 1128/1128 [00:08<00:00, 128.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 105928.49407185947\n",
      "R^2: 0.524054047867564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Enter your code here\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "X_tab_test = tab_preprocessor.transform(test_data)\n",
    "preds = trainer.predict(X_tab=X_tab_test, batch_size=64)\n",
    "\n",
    "print(\"RMSE:\", mean_squared_error(preds, test_data['resale_price'].values, squared=False))\n",
    "print(\"R^2:\", r2_score(preds, test_data['resale_price'].values))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
